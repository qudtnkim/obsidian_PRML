1.1. 곡선피팅
#PRML_Introduction 
 
> **목표** : 실수 범위의 입력 변수 $x$ 를 관찰한 후 이 관찰 값을 바탕으로 실수 범위의 타겟(target) $t$ 의 값을 예측하고 싶다. 
> 훈력 집합들을 사용해서 어떤 새로운 입력값 $\hat x$  가 주어졌을 때, 타깃 변수 $\hat t$ 를 예측하는 것이다. 

## [[1.2. Probability Theory]]
불확실성을 정량화시켜 표현할 수 있는 수학적인 프레임워크를 제공

## [[1.5. The Decision Theory]]
확률적 표현을 바탕으로 적절한 기준에 따라 최적의 예측을 수행할 수 있는 방법론을 제공한다.

구체적으로는 입력데이터 $\hat x$ 를 가지고 예측 결과 $\hat t$ 를 출력하는 모델을 만드는 것으로 다음 처럼 모델링 한다.
$$y(x,{\bf w})=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum_{j=0}^{M}w_jx^{\;j} \qquad{(1.1)}$$
M은 이 다항식의 차수 이며 x의 j 제곱이 있다.

다항식의 계수 $w_{0}$ 부터 $w_{M}$  을 모아서 벡터**w** 로 표현할 수 있겠다. 

다항함수 y(x,W) 는 x에 대해서는 비선형이지만, 계수 w에 대해서는 선형이다. 다항 함수와 같이 알려지지 않은 변수에 대해 선형인 함수들은 선형 모델이라 불린다. 

훈련 집합의 표적값들의 값과 함숫값 y(x,w)와의 오차를 측정하는 오차 함수를 정의하고, 이 함수의 값을 최소화하는 방식으로 데이터의 ==곡선을 피팅한다.== 

$$E({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}\{y(x_n,{\bf w})-t_n\}^2 \qquad{(1.2)}$$
가장 흔하게 쓰이는 #오차함수 가 1.2와 같이 주어진다.
$\frac {1}{2}$ 는 이후의 편의를 위해서 추가되었다. 함수 y(x,w) 가 정확히 데이터 포인트 들을 지날 때만 오차의 값이 0이 된다. 


$$E_{RMS}=\sqrt{\frac{2E({\bf w}^*)}{N}} \qquad{(1.3)}$$
$w^{*}$ 은 유일해를 뜻함. 

- 에러 E 를 최소화하는 모델로써 RMS 값이 작을 수록 더 적합한 모델이다. RMS 값을 샘플 크기인 N으로 나누는 이유는? 
- 학습데이터와 테스트데이터가 서로 다른 데이터 크기를 가지는 경우에 발생하는 스케일 문제를 보정하기 위한 정규화 요소이다. 

![[Pasted image 20220902142801.png]]

9차원의 곡선으로 피팅한 모델을 사용하고자 하였을 때, 데이터에 대해 일반화성능을 확보하지 못하였으며, 오버 피팅이 되었다고 말할 수 있다.


![[Pasted image 20220902145159.png]]

위의 그림은 차수 M=9 인 데이터로 샘플 데이터의 개수에만 차이가 존재한다. 왼쪽은 15개 오른쪽은 샘플크기 N = 100인 모델이다. 

적은 수의 샘플을 사용한 경우에 M=9에서 오버피팅이 발생하였는데, 데이터 크기가 큰 오른쪼그이 경우 이러한 현상이 없는 것 같다.

> 관찰 데이터의 크기가 클수록 오버 피팅을 막을 수 있다.

- 물론 해결하고자 하는 문제에 따라서 모델의 복잡도를 다르게 해야하지만, 휴리스틱 관점에서는 모델 파라미터의 개수는 샘플 크기의 1/5, 1/10 보다 작게 하는 것이 좋다. 

>파라미터 개수를 효율적으로 결정하는 방법은 [[베이지안 정리]]- 베이지안 접근 방식을 통해 자세히 알아보도록 한다.

## 데이터는 적으나 가급적 높은 차수의 파라미터를 가진 모델을 만들어낼 방법 = 모델의 복잡도는 올리고, 오버피팅은 가급적 피하는 기법
==정칙화(Regularization)==
$$\tilde{E}({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}\{y(x_n,{\bf w})-t_n\}^2+\dfrac{\lambda}{2}\|{\bf w}\|^2\qquad{(1.4)}$$
$\bf ||w||^{2}$ $\equiv$ $w^{T}w \equiv w^{2}_{0} + w^{2}_{1} +...+ w^{2}_{M}$ 는 정칙자( #regularizer )라고 함. 

계수 $\lambda$ 가 매우 중요한 요소로서 정칙화 텀, 정칙화 계수라고 부름

위의 식은 닫힌 구조로써 명확한 해를 구할 수 있는 식이라는 데에는 변함이 없다.

정칙화 요소 w 가 quadratic 2차식 형태라면 Ridge 정칙화라고 부른다.
 
### 출처(참고문헌)
- Pattern Recognition and Machine Learning = Christopher M. Bishop
- http://norman3.github.io/prml/

### 연결문서
[[1. Introduction]]