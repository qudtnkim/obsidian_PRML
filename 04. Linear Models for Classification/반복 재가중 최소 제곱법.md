iterative reweighted least squares

#IRLS

반복적 재간중 최소 제곱법 IRLS 는 p-규격 형태으의 객관적 함수로 특징 최적화 문제를 해결하는 데 사용된다.

가중 최소 제곱 문제 해결을 반복하는 방법

IRLS 는 일반화된 선형 모델의 최대가능도 추정을 위해 사용됨.


 # 분류
 분류 문제를 풀기위한 문제에서 Logistic regression은 기본적인 예시가 된다고 할 수 있겠다.

분류란 예측하고자 하는 대상이 discrete 한 class를 가지고 있음을 의미하여, 클래스 $C_{1}$ 과 클래스 $C_{2}$ 를 가지고 있다고 해보겠다. 

두 변수 사이에서 사전확률과 사후확률 사이의 관계를 표현하는 [[베이지안 정리]] 를 가져다 쓴다면,

$C_{2}$ 의 확률 P($C_{2}|x$) = P($C_{1}|x$)
##   $$ P(C_{1}|x) = \frac {P(x|C_{1})P(C_{1})}{P(x)}$$
##  $$P(C_{1}|x) = \frac {P(x|C_{1})P(C_{1})}{P(x|C_{1})P(C_{1})+P(x|C_{2})P(C_{2})}$$
##  $$P(C_{1}|x) = \frac {1} {1+ \frac {P(x|C_{2})C_{2}}{P(x|C_{1})C_{1}}}$$
##  $$P(C_{1}|x) = \frac{1}{1+exp(-\xi)} $$

의 형태로 나오며 exp 와 상쇄되도록 log를 취한 형태로써 아래와 같다.

## $\xi$  = -log $\frac {P(x|C_{1})}{P(x|C_{2})}$ - log $\frac {C_{1}}{C_{2}}$


$\xi$ 의 앞의 항을 likelihood ratio (공산비), 뒤의 항을 prior ratio 라고 함.

위의 함수는 로지스틱 함수라고 할 수 있음.

함수의 형태는 시그모이드 함수 형태로서 $\xi$ 가 크면 class 1 일 확률이 크며, P($C_{1}$|x) = 1 로 수렴함을 뜻하는 것임.


# Maximum likelihood Formulation
[[최대가능도]] 
위와 같이 모델을 정의 했다면, weight 를 업데이트하기 위해서 optimization 과정이 필요하다. 

현재의 모델이 잘 예측하는가에 대한 metric이 필요함을 뜻하는데, 다음과 같이 풀어나간다.

logistic regresion 은 y가 discrete value 로 이루어져 있을 테니 P(y|X) 가 $Bernoulli$ 분포를 따른다.

### random variable X $\in$ {1,0} 으로 discrete value를 가지고 P(X=1) = $\mu$ , P(X=0) = 1 - $\mu$
의 특성을 가질 때 $Bernoulli$ 분포라 하며 아래와 같이 쓴다.

# $$Bern(x;\mu)= \mu^{x}(1-\mu)^{1-x}$$  
확률 질량함를 위와 같이 작성 한다.

다시 돌아와 위의 베르누이 정의를 활용하면 logitic regression 의 확률분포를 아래와 같이 표현 가능하다.

# $$P(y=1|x)^{y}(1-P(y=1|x))^{(1-y)}$$
위 식이 파라미터를 가지고 정의한 모델에 적용된다면 아래와 같이 고칠 수 있다.

# $$\sigma(W^{T}x)^{y}(1-\sigma(W^{T}x)) ^{(1-y)}$$
위의 식이 likelihood function #가능도함수 라고 할 수 있다.

모든 확률의 나열 곱이므로 

# $$p(y|X, w)= \prod^n_{i=1}\sigma(W^{T}x)^{y}(1-\sigma(W^{T}x))^{(1-y)}$$
## $$ L = \Sigma^{N}_{n=1}(logP(y|x_{n}) = \Sigma^{N}_{n=1}({y_{n}log\hat{y}_{n}}+(1-y_{n})log(1-\hat y_{n})) $$
### where $\hat y_{n}$ = $\sigma (W^{T}X_{n})$

**w** 를 파라미터로 갖는 log- 가능도 함수를 작성하였으며, 목적인 위의 식에 대해서 최대화 할 수 있는 w 파라미터 벡터를 찾는 것으로 돌아가면 미분을 적용할 수는 없음. 

시그모이드 함수의 선형성을 보장할 수 없기 때문.

==따라서 재가중적인 방법으로 w 를 조금씩 update 함.==

# IRLS
[[반복 재가중 최소 제곱법]]