# 판별 모델 vs 생성모델
판별모델과 생성모델의 차이를 보면,
판별 모델은 데이터 x가 주어졌을 때, label y가 나타날 조건부 확률을 직접 반환하는 모델임. (Classification)
지도학습 모델이라고 할 수 있는 것.


# 확률적 판별 모델

- 2-class 문제에서는 사후확률을 p($C_{k}$|x) 시그모이드 함수로 표현한다. 
- k-class 문제에서는 사후확률을 soft max로 제동한다. (K>2 의 경우를 말함.)
  [[소프트 맥스]] 함수는 시그모이드 함수로 부터 유도되었다. 

==다른 방식의 확률적 판별모델의 선택을 하기 위한 방도를 모색함==

## HOW?
- 선형 모델 함수 형태를 명시적으로 사용하고 [[최대가능도]] 방법을 적용해서 직접 구한다.
- 특정 현상이 발생했다고 할 때, 통계모델을 통해서 가장 특정현상이 일어날 법 했을 경우의 해당 통계모델의 최댓값을 찾는 것임. 
- [[반복 재가중 최소 제곱법]] 이 해를 구하기 위해 효율적이다. (IRLS)


## 4.3.1 고정된 기저 함수

==선형회귀에서는 입력 변수를 선형함수로 모델링한다. 그러나 비선형 데이터에 대해서는 이와 같은 회귀 모형이 적절치 못하다. 기저함수란 이러한 비선형 데이터를 표현하기 위해 일정특징을 가지고 새로운 함수를 새로이 만들어낼 수 있는 방법이다==

 기저함수를 지난 공간에서는 선형으로 분리되는 데이터(클래스)들이 원래의 공간 상에서도 선형으로 분류가 되는 것은 아니며, 본래의 공간의 데이터들의 중첩 정도를 증가 시키기도 한다.

그럼에도 기저함수를 사용하는 이유는 적절한 기저함수의 선택을 통해서 사후 확률 값을 더 쉽게 모델링 할 수 있기 때문이다.


   
## 4.3.2 로지스틱 회귀 (Logistic Regression)
- Binary Classification 문제에서 일반화된 선형 모델에 대해서 알아보고자 한다. $C_{1}$ 에 대한 사후 확률 값이 로지스틱 회귀로 기술 될 수 있음을 확인하였고, 

$$p(C_1|\phi)=y(\phi)=\sigma({\bf w}^T\phi) \qquad{(4.87)}$$

## $P(C_{1}|\phi) = y(\phi) = \sigma(W^{T}\phi)$ 에서 부터

## $P(C_{2}|\phi) = 1 - P(C_{1}|\phi)$

## $\sigma$는 로지스틱 시그모이드라고 부른다. sigmoid

## 기저 함수 $\phi$ 가 M 차원 공간을 가지면, 이 모델은 조정 가능한 모수를 M차원으로 가지게 된다.

(가우시안 클래스-조건부 밀도에서라면 더 많은 모수를 요구할 것 이다.)

로지스틱 회귀 모델의 모수 값을 결정하기 위해 [[최대가능도]]추정 (=MLE) 를 사용함.

- 우리는 로지스틱 회귀 모델의 모수 값을 결정하기 위해 MLE를 사용할 것이다.
- 이 과정에서 로지스틱 함수를 미분할 필요가 있으며 미분하면 아래의 식을 얻는다.

$$\dfrac{d\sigma}{da} = \sigma(1-\sigma) \qquad{(4.88)}$$

- 식 자체는 어렵지 않다. 간단하게 증명하면,

$$\sigma(a)=\dfrac{1}{1+\exp(-a)}$$

 $$\dfrac{d\sigma}{da}=\dfrac{\exp(-a)}{(1+\exp(-a))^2} = \sigma(a)\left\{\dfrac{\exp(-a)}{1+exp(-a)}\right\}=\sigma(a)\left\{1-\dfrac{1}{1+\exp(-a)}\right\} $$
$$= \sigma(1-\sigma)$$
- binary class 문제를 다루므로 확률 $C_{1}$ 에 대한 확률과 $C_{2}$ 에 대한 확률 곱을 독립사건으로 보고 반복 횟수 만큼을 곱한다면 다음의 #가능도함수 를 얻는다.

$$p({\bf t}\;|{\bf w}) = \prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n} \qquad{(4.89)}$$

### where $y_{n} = p(C_{1}|\phi_{n})$


- MLE 추정: 계산 편의상 양변에 로그를 취해 합으로 표현하였고, [[뉴턴 라프슨 Newton-Raphson]] 법을 적용하고자 한다면 Convex한 함수이어야하기에 Negative 항으로 전환하고 이를 (가능도함수의 계산 편의를 위해 조작한 형태를) 오류함수로 정의한다.

$$E({\bf w})= -\ln{p({\bf t}|{\bf w})} = - \sum_{n=1}^{N}\left\{t_n\ln{y_n}+(1-t_n)\ln(1-y_n)\right\} \qquad{(4.90)}$$
위의 형태를 Cross-entropy error function이라고 한다.

![[Pasted image 20220901005917.png]]

![[Pasted image 20220901005931.png]]
![[Pasted image 20220901005944.png]]

- 최대 가능도함수를 찾는 것이 목적이었으므로 Gradient 를 얻기 위해서 Cross Entropy 함수를 미분하면,
$$\triangledown E({\bf w})=\sum_{n=1}^{N}(y_n-t_n)\phi_n \qquad{(4.91)}$$
오류함수의 Gradient를 얻을 수 있음.
$$\frac{\partial E}{\partial y_n} = \frac{1-t_n}{1-y_n} - \frac{t_n}{y_n} = \frac{y_n-t_n}{y_n(1-y_n)}$$
$$\frac{\partial y_n}{\partial a_n} = \frac{\partial \sigma(a_n)}{\partial a_n} = \sigma(a_n)(1-\sigma(a_n)) = y_n(1-y_n)$$
$$\nabla a_n = \sigma_n$$
$$\nabla E({\bf w})=\sum_{n=1}^{N}(y_n-t_n)\phi_n$$

## 4.3.3 IRLS (Iterative reweighted least squares)
- 이제 3장에서 소개한 선형 회귀 모델에서의 MLE를 잠시 떠올려보자.
- [[반복 재가중 최소 제곱법]] 

- 이제 까지의 선형 회귀 모델에서의 [[최대가능도]] 추정을 살펴보면, 가우시안 노이즈 모델을 선정하여 식을 모델링 했었다.
- 이 식은 닫힌 형태 이므로 최적의 해를 구할 수 있었으며 모수 벡터에 대해 2차식의 형태로 제공되기에 최소값이 1개로 도출가능 했었으나, 로지스틱 회귀에서의 식은 닫힌 형태의 식이 아니다.

- ==시그모이드 식에 의해 비선형 모델이 된다.==
- 
- convex 함수인 에러 함수에 대해 미분하여 최소점을 찾지 않을 수 없으니 사용하는 방법이, Iterative 하게 w 를 업데이트 하면서 최적을 값 가능도 함수의 기여도의 최댓값을 찾을 수 있는 점을 찾아야하며, Newton법 혹은 뉴턴 랩슨 법이라고 불리는 방법을 사용한다.
![[Pasted image 20220901013925.png]]

convex 함수라면 계속해서 현재 x 값에서 접선을 그리고 접선이 x축과 만나는 지점으로 x를 이동시켜 가면서 점진적으로 해를 찾는 방법이다.

뉴턴 랩슨 식
$${\bf w}^{(new)}={\bf w}^{(old)}-{\bf H}^{-1}\nabla E({\bf w}) \qquad{(4.92)}$$
H는 헤시안 행렬이고 오차 함수의 2차 미분값으로 정해 진다.

$$y({\bf x}, {\bf w}) = \sum_{j=0}^{M-1} w_j \phi_j({\bf x}) = {\bf w}^T \phi({\bf x}) \qquad{(3.3)}$$
$$E_D({\bf w}) = \frac{1}{2}\sum_{n=1}^{N} \{ t_n - {\bf w}^T\phi({\bf x}_n) \}^2 \qquad{(3.12)}$$

- 따라서 이 함수의 그라디언트와 헤시안은 다음과 같다.

$$\nabla E({\bf w}) = \sum_{n=1}^{N} ({\bf w}^T \phi_n - t_n)\phi_n = \Phi^T\Phi{\bf w} - \Phi^T{\bf t} \qquad{(4.93)}$$

$${\bf H} = \nabla\nabla E({\bf w}) = \sum_{n=1}^{N} \phi_n \phi_n^T = \Phi^T \Phi \qquad{(4.94)}$$

4.92 뉴턴랩슨법에 위의 오차함수 그래디언트와 헤시안을 대입하면 아래의 식을 얻는다.

$${\bf w}^{(new)} = {\bf w}^{(old)} - (\Phi^T\Phi)^{-1} \left\{ \Phi^T\Phi {\bf w}^{(old)}-\Phi^T {\bf t} \right\} = (\Phi^T\Phi)^{-1}\Phi^T{\bf t} \qquad{(4.95)}$$


# 적용

- 이제 *Newton-Raphson* 업데이트를 로지스틱 회귀에 적용해보기 위해 cross-entropy 오차함수에 대입을 해보도록 하자. (식 4.91로 부터 오류 함수의 기울기와 헤시안을 다음과 같이 얻을 수 있다.)
$$\nabla E({\bf w}) = \sum_{n=1}^{N} (y_n - t_n)\phi_n = \Phi^T({\bf y}-{\bf t}) \qquad{(4.96)}$$
$${\bf H} = \nabla\nabla E({\bf w}) = \sum_{n=1}^{N} y_n(1-y_n) \phi_n \phi_n^T = \Phi^T {\bf R} \Phi \qquad{(4.97)}$$

- R 은 N x N인 대각 행렬로 아래와 같다.

$$R_{nn} = y_n(1-y_n) \qquad{(4.98)}$$
헤시안이 가중 행렬 R 로 인해서 W에 종속성을 갖게 되었다.

$y_{n}$ 이 0과 1사이의 수라는 사실로 부터 H의 속성을 이해할 수 있다.
- 임의의 벡터 u에 대해 $u^{T}Hu>0$ 를 만족한다.
- 헤시안 행렬은 양의 정부호 행렬이고 다라서 w 에 대한 볼록 함수 이다.

 $${\bf w}^{(new)} = {\bf w}^{(old)} - (\Phi^T{\bf R}\Phi)^{-1}\Phi^T({\bf y}-{\bf t})\\
$$
$$= (\Phi^T{\bf R}\Phi)^{-1}\left\{\Phi^T{\bf R}\Phi{\bf w}^{(old)}-\Phi^T({\bf y}-{\bf t})\right\}\\
= (\Phi^T{\bf R}\Phi)^{-1}\Phi^T{\bf R}{\bf z} \qquad{(4.99)}$$

뉴턴 랩슨법을 업데이트하면 4.99의 식을 얻음.



==그 모양이 linear regression 에서 least squared method로 도출한 weight 식과 유사하며, 위의 식에서 w는 업데이트 시마다 이에 종속적인 값인 S,b가 바뀌기를 반복하며, w의 변화가 없을 때까지 위의 과정을 반복한다고 하여== IRLS
![[Pasted image 20220901024532.png]]
(linear regression least squared method)


- N차원 벡터 z는 다음을 원소로 가진다.
$${\bf z} = \Phi{\bf w}^{(old)} - {\bf R}^{-1}({\bf y}-{\bf t}) \qquad{(4.100)}$$


이렇게 가중치가 부여된 최소 제곱 문제는 가중치 대각 행렬 R 을 분산 값으로 생각할 수 있는데 로지스틱 회귀 문제에서 t의 평균과 분산값이 다음으로 주여져있기 때문임.

$$E[t] = \sigma({\bf x}) = y \qquad{(4.101)}$$

$$var[t] = E[t^2] - E[t]^2 = \sigma({\bf x}) - \sigma({\bf x})^2 = y(1-y) \qquad{(4.102)}$$

t $\in{0,1}$ 이므로 $t^{2} = t$ 를 사용하였음.

IRLS 는 변수 a=$W^{T}\phi$ 에 대해 선형문제로 해석이 가능하며, Z 의 n 번째 원소인 $z_{n}$ 는 현재 사용하고 있는 $W^{old}$ 주변에 대해 로지스틱 시그모이드 함수를 지역적으로 선형 근사하여 구한 공간상에서의 실제적인 표적값이라고 해석 할 수 있다고 함.

$$a_n({\bf w}) \simeq a_n({\bf w}^{(old)}) + \left. \frac{da_n}{dy_n}\right|_{ {\bf w}^{(old)} }(t_n-y_n)\\
= \phi_n^T {\bf w}^{(old)} - \frac{y_n-t_n}{y_n(1-y_n)}=z_n \qquad{(4.103)}$$


## 4.3.4 다중 로지스틱 회귀 (Multiclass logistic regression)
- 다중 클래스 분류 문제를 다루는 Generative 모델에서는 선형 함수인 소프트맥스(softmax)가 사용된다고 이야기했다.

$$p(C_k|\phi) = y_k(\phi) = \frac{\exp(a_k)}{\sum_j \exp(a_j)} \qquad{(4.104)}$$
- 클래스 조건부 분포와 사전 분포를 따로 구하기 위해서 최대 가능도 추정이나 베이지안 정리를 사용해서 사후확률을 찾았는데, 이는 간접적으로 매개변수 $W_{k}$ 를 구하는 방법이며 직접구하기 위해서는 $y_{k}$ 에 대한 미분값이 필요함.

# $$\frac{\partial y_k}{\partial a_j} = y_k(I_{kj}-y_i) \qquad{(4.106)}$$
- 여기서 활성자(activations) $a_{k}$ 는 다음과 같이 정의된다.

$$a_k = {\bf w}_k^T \phi \qquad{(4.105)}$$

- 여기서 $I_{kj}$ 는 단위 행렬(identity matrix) 이다.

- 로지스틱회귀에서와 유사하게 각 클래스의 각 사전확률들의 곱을 통해 가능도 함수를 구한다.

- 다중 클래스에 대한 가능도 함수를 구하는 데에는 다음 처럼 표현한다.
$$p({\bf T}|{\bf w}_1,...{\bf w}_K) = \prod_{n=1}^{N}\prod_{k=1}^{K} p(C_k|\phi_n)^{t_{nk}} = \prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}} \quad{(4.107)}$$
- 이 때 $y_{nk} = y_{k}(\phi_{n})$ 이고 T 는 N x K인 행렬이다.
- 음의 로그값을 취하고 크로스 엔트로피를 얻는다.

$$E({\bf w}_1, ..., {\bf w}_K) = -\ln p({\bf T}|{\bf w}_1, ...,{\bf w}_K) = - \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk}\ln(y_{nk}) \qquad{(4.108)}$$

그래디언트를 얻어보면 다음과 같다.
$$\nabla_{ {\bf w}_j }E({\bf w}_1, ...,{\bf w}_K) = \sum_{n=1}^{N} (y_{nj}-t_{nj})\phi_n \qquad{(4.109)}$$
 
## 4.3.5 프로빗 회귀 (Probit regression)
- 지금가지 우리는 클래스-조건부 분포가 지수족 분포를 따를 때의 사후 분포를 로지스틱 회귀를 이용하여 구하는 것을 살펴보았다.

- (정규 분포, 다항 분포, 감마 분포 등)

- 이 때 선형 함수에 로지스틱 혹은 소프트맥스 변환을 통해 사후 분포를 예측하였다.
- 그러나 클래스-조건부 분포로 선택할 수 있는 분포가 모두 간단한 사후 확률을 가지는 것은 아니다.
    - 예를 들어 가우시안 혼합 분포를 사후 분포로 사용하는 경우 이런 모양을 얻을 수 없다.
    - 혹은 p 값이 0~1 사이에서 확보되지 않는 경우, 일반 해석하기 어려움.

- 따라서 여기서는 discriminative 확률 모델의 또 다른 형태를 살펴보도록 한다.
    - 2개의 분류 분제를 가진 일반화된 선형 모델(generalized linear modle) 프레임워크를 고려한다.
    
$$p(t=1|a) = f(a) \qquad{(4.111)}$$

- 이 때 \\( a={\bf w}^T\phi \\) 이고, 함수 \\( f(\cdot) \\) 는 활성 함수(activation function)가 된다.
- 여기서 링크(활성) 함수를 *noisy threshold model* 로 고려해볼 수 있다.
    - 이 함수는 입력 \\( \phi\_n \\) 을 \\( a\_n={\bf w}^T\phi\_n \\) 으로 평가한 다음 이를 다음처럼 해석할 수 있다.

$$\left\{\begin{array}{lr}t_n=1 & if\;a_n\ge \theta \\t_n=0 & otherwise\end{array}\right. \qquad{(4.112)}$$

- 여기서 \\( \theta \\) 는 고정된 값이 아니라 랜덤 변수로 취급한다.
- \\( \theta \\) 가 확률 밀도 \\( p(\theta) \\) 를 가진다면 활성 함수는 다음과 같이 기술할 수 있다.

$$f(a) = \int_{-\infty}^{a}p(\theta)d\theta \qquad{(4.113)}$$

- 이는 조건식에 의해 \\( P(a\ge \theta) \\) 를 나타내는 식이 되기 때문이다.

![figure4.13]({{ site.baseurl }}/images/Figure4.13.png){:class="center-block" height="200px"}

- 여기서 파란색 라인은 \\( p(\theta) \\) 로 두 개의 가우시안 함수의 혼합으로 이루어진 것을 알 수 있다.
- 붉은색 라인은 CDF 인 \\( f(a) \\) 로 대충 시그모이드 함수와 비슷하게 생겼다.
- \\( x \\) 축은 \\( \theta \\) 의 값으로 연두색 영역의 직선 위치가 바로 \\( a \\) 라고 생각하면 된다.
- 이제 \\( p(\theta) \\) 를 표준 정규 분포 (평균이 \\( 0 \\) 이고 분산이 단위 분산 \\( 1 \\) 인 가우시안 확률 분포)로 생각한다면 식을 다음과 같이 기술할 수 있다.

$$\Phi(a) = \int_{-\infty}^{a}N(\theta|0, 1)d\theta \qquad{(4.114)}$$

- 이 함수를 역 프로빗 함수(inverse probit function)라고 부른다.
    - 시그모이드와 거의 유사한 형태의 값을 가진다. 그림 4.9를 참고하면 된다.
- 실제 구현체에서는 다음과 같은 식을 주로 사용한다.

$$erf(a) = \frac{2}{\sqrt{\pi}} \int_{0}^{a} \exp(-\theta^2) d\theta \qquad{(4.115)}$$

- 이 함수는 보통 \\( erf \\) 함수 또는 에러 함수라고 부른다.
    - 이 때 "에러 함수"의 의미가 기계 학습에서의 에러 함수를 의미하는 것이 아니니 유의할 것
- \\( erf \\) 함수를 이용하여 역 프로빗 함수를 전개하면 다음 식을 얻을 수 있다.

$$\Phi(a) = \frac{1}{2} \left\{1+erf\left(\frac{a}{\sqrt{2}}\right)\right\} \qquad{(4.116)}$$

- 이 식을 프로빗 회귀 (probit regression)식이라고 부른다.

---

- MLE를 이용하여 모델의 파라미터 값을 결정할 수 있다.
    - 이 때 프로빗 회귀는 로지스틱 회귀와 유사한 결과를 얻게 된다.
    - 실제 4.5 절에서 프로빗 모델의 사용법을 확인할 것이다.
- 실제 문제에서는 고려해야 할 사항으로 outliers 가 있다.
    - 입력 벡터 \\( s \\) 를 측정할 때 에러가 발생하거나 타겟 값 \\( t \\) 가 잘못 부여된 경우에 발생할 수 있다.
    - 이런 데이터가 존재하는 경우 분류 결과를 심각하게 왜곡할 수 있음
    - 로지스틱 모델과 프로빗 모델은 이러한 상황에서 다르게 동작하게 된다.
        - 로지스틱 시그모이드의 경우 \\( x\rightarrow\infty \\) 이면 활성화 함수의 값이 \\( \exp(-x) \\) 와 같이 급격하게 줄어든다.
        - 반면 프로빗 활성화 함수는 \\( \exp(-x^2) \\) 과 같이 줄어들기 때문에 outlier에 훨씬 민감함
- 하지만 로지스틱과 프로빗 모델 모두 데이터는 모두 정확하게 라벨링되어 있다고 가정하는 모델이다.
    - 물론 잘못된 라벨의 영향도를 타겟 값 \\( t \\) 가 잘못된 값으로 할당될 확률 \\( \epsilon \\) 로 표현하여 모델 요소로 포함할 수 있다.
    - 이 경우 타겟 값에 대한 확률 분포는 다음과 같이 기술할 수 있다.
    
$$p(t|{\bf x}) = (1-\epsilon)\sigma({\bf x}) + \epsilon(1-\sigma({\bf x})) = \epsilon + (1-2\epsilon)\sigma({\bf x}) \qquad{(4.117)}$$


## 4.3.6 정준 연결 함수 (Canonical link functions)
- 가우시안 노이즈 분포를 사용한 선형 회귀 모델에서 에러 함수는 음 로그 가능도 함수(negative log likelihood)를 사용한다. (식 3.12)
- 참고로 (식 3.12)는 다음과 같다.

$$E_D({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}\{t_n-{\bf w}^T\phi(x_n)\}^2 \qquad{(3.12)}$$

- 위의 식에서 \\( y\_n = {\bf w}^T\Phi\_n \\) 이며 오차함수를 파라미터 \\( {\bf w} \\) 로 미분한 결과로 부터 식을 유도한다.
    - 로지스틱 시그모이드 활성 함수 또는 softmax 활성 함수를  cross-entropy 오차 함수를 결합하는 식에서도 비슷한 것을 확인함
- 이제 이러한 식들을 *exponential family* 식으로 전개를 시켜보도록 하자.
    - 이 때의 활성 함수를 정준 연결 함수 (canonical link function) 이라고 한다.
    - 한국말로 번역한 것이 좀 이상하므로 그냥 영어로 표현해서 사용하는 것이 더 좋을듯 하다.
    - 일단 타겟 변수의 확률 식을 표현해보자.
    
$$p(t|\eta, s) = \frac{1}{s}h\left(\frac{t}{s}\right)g(\eta)\exp\left(\frac{\eta t}{s}\right) \qquad{(4.118)}$$

- 4.2.4절에서는 입력 데이터 \\( {\bf x} \\) 에 대해 *exponential family* 분포를 가정했지만 여기서는 타겟 값 \\( t \\) 에 대해 가정한다.
- 식 (2.226) 을 참고하도록 하자.

$$y \equiv E[t|\eta] = =s\frac{d}{d\eta}\ln g(\eta) \qquad{(4.119)}$$

- 일반화된 선형 모델 ( *generalized linear model* , *Nelder & Wedderburn (1972)* ) 식은 다음과 같다.

$$y = f({\bf w}^T \phi) \qquad{(4.120)}$$

- 여기서 함수 \\( f( \dot ) \\) 는 활성 함수 ( *activation function* )로 알려져있다.
    - 그리고 이 때 \\( f^{-1}( \dot ) \\) 이 바로 연결 함수( *link function* )이다.
- 이제 식 (4.118) 의 *log likelihood* 함수를 \\( \eta \\) 의 함수로 표현해 본다.

$$\ln p({\bf t}|\eta, s) = \sum_{n=1}^N \ln p(t_n|\eta, s) = \sum_{n=1}^N \left\{ \ln g(\eta_n) + \frac{\eta_n t_n}{s} \right\} + const \qquad{(4.121)}$$

- 모든 관찰값이 동일한 스케일(scale) 파라미터를 공유한다고 가정한다.
    - 따라서 \\( s \\) 는 \\( \eta \\) 에 대해 독립적이다.
- 모델 파라미터 \\( {\bf w} \\) 에 대해 미분하면, 

$$\nabla_{\bf w} \ln p({\bf t}|\eta, s) = \sum_{n=1}^N \left\{ \frac{d}{d\eta_n}\ln g(\eta_n) + \frac{t_n}{s} \right\} \frac{d\eta_n}{dy_n} \frac{dy_n}{da_n}\nabla a_n = \sum_{n=1}^N \frac{1}{s} (t_n-y_n)\psi'(y_n)f'(a_n)\phi_n \qquad{(4.122)}$$

- 여기서 \\( a\_n = {\bf w}^T \phi\_n \\) 이고, \\( y\_n = f(a\_n) \\) 이다.

$$f^{-1}(y) = \psi(y) \qquad{(4.123)}$$

- \\( f(\psi(y)) = y \\) 이고 따라서 \\( f'(\psi)\psi'(y) = 1 \\) 이다. 
- 또한 \\( a=f^{-1}(y) \\) 이므로 \\( a=\phi \\) 이고 결국 \\( f'(a)\psi'(y) = 1 \\) 이 된다.

$$\nabla E({\bf w}) = \frac{1}{s} \sum_{n=1}^{N} \{y_n-t_n\}\phi_n \qquad{(4.124)}$$

### 출처(참고문헌)
- Pattern Recognition and Machine Learning = Christopher M. Bishop
- http://norman3.github.io/prml/
- https://angeloyeo.github.io/2020/07/17/MLE.html
### 연결문서
[[0. Linear Models for Classification]]


- Binary Classification 의 경우에, 조건부 분포에 대하여 $C_{1}$ 의 사후확률을 x 의 선형 함수에 대한 로지스틱 시그모이드 함수로 표현함.
- *p*(x|*$C_{k}$)



- 클래스가 여럿인 경우에는 K 번째 클래스에 대한 사후확률을 표현할 때, x에 대한 선형함수를 소프트맥스 함수로 변환하여 표현함.

- 클래스 조건부 밀도가 특정한 분포를 가지고 있을 경우에는 [[최대가능도]] 방법을 이용하여 밀도의 매개변수와 클래스 사전 분포  P(Ck)를 구하고, 거기에 [[베이지안 정리]]를 적용해서 사후 클래스 확률을 구함.

- 대안으로는 일반화된 선형 모델의 함수 형태를 명시적으로 사용하고, 여기에 최대 가능도 방법을 적용해서 함수의 매개변수를 직접 구함. [[반복 재가중 최소 제곱법]]

일반화된 선형 모델의 매개변수를 찾을 때, 클래스 조건부 밀도와 클래스 사전 분포를 따로 피팅하고 [[베이지안 정리]] 를 적용하여 간접적으로 매개변수를 찾는 것은 [[생성적 모델링]] 의 예시이다.

이에 반해서 직접적인 접근법으로는 조건부 분포 $p(C_{k}|x)$  를 통해 정의된 #가능도함수 를 직접 극대화 한다. 이는 [[판별적 훈련]]의 예시 이다.

---------------

## 고정된 [[기저 함수]]

원입력 벡터 x 에 대해 직접 적용되는 분류 모델들은 기저 함수들의 벡터를 이용하면, 입력 값에 대해 고정된 비선형 변환을 먼저 적용한 후에도 사용할 수 있다. 이결과로 얻어지는 결정 경계는 특징 공간상에서 선형이며, 이 결정 경계는 원 x 공간에서는 비선형일 것 이다. (*이게 뭔소리냐*)

특징 공간 상에서 선형적으로 분리 가능한 클래스들이 원래의 관측 공간에서도 선형적으로 분리되는 것은 아니다.(*당연*)

회귀를 위한 선형 모델에 대한 논의에서 처럼 기저 함수들 중 하나는 보통 특징공간 $파이_{o}(x)=1$
과 같이 상숫값을 가지도록 설정된다.

![[Pasted image 20220829124444.png]]
그림 4.12 는 비선형 기저 함수에서 변환된 데이터가 선형 판별되는 것을 묘사한다.

p($C_{2}|\phi$) = 1 - p($C_{1}|\phi$)
$$p(C_{1}|\phi) = y(\phi) = \sigma(w^T\phi)$$

$\sigma( )$ 는 로지스틱 시그모이드 함수임. 
위의 모델은 [[로지스틱 회귀]]라고 불리며, 분류를 위한 모델임.

[[최대가능도]] 방법을 이용해서 로지스틱 회귀 모델의 매개변수를 구하면, 이를 위해서는 로지스틱 시그모이드 함수의 미분값을 이용해야함. 로지 스틱 시그모이드의 미분값에는 시그모이드 함수의 미분값이 포함된다.

$$d\sigma/da = \sigma(1-\sigma)$$
n = 1 ,...,N 에 대해서 $t_{n}$ 은 {0,1} 
타겟 클래스가 0 혹은 1인 문제에서 $\phi_{n} = \phi(x_{n})$ 인 데이터 집합 들에서 {$\phi(x_{n}), t_{n}$} 에 대해서 가능도 함수를 적는 다면 다음과 같다.
![[Pasted image 20220829151710.png]]


크로스 엔트로피의 곱
(($\pi$) 는  요소 곱을 의미한다.)

$y_{n} = p(C_{1}|\Phi_{n})$

기저함수를 만족할 때, 클래스 1 일 확률이 $y_{n}$ 임

가능도 함수의 음의 로그값을 취하면 오류 함수가 정의 됨. -> cross entropy 함수임

선형적으로 분리 가능한 데이터 집합에 대해 최대 가능도 방법을 사용하면 심각한 과적합 문제를 겪는다. 

$\sigma = 0.5$ 에 해당하는 초공간이 두 클래스를 나누는 경우에 발생한다. 이 때, $w^{T}\phi = 0$ 에 해당하며, w 의 크기는 무한대가 된다. 이 경우 특징 공간 상에서의 로지스틱 회귀 함수는 무한대로 가팔라지며, 따라서 헤비사이드 계단 함수가 된다. 각각의 클래스 k 에서 온 모든 훈련 포인트 들이 [[사후 확률]] p($C_{k}|x)=1$ 을 가지게 됨.  

또한, 해들의 연속체가 존재하게 되는데, 초공간이 클래스들을 나눈다 하더라도 훈련 집합의 데이터 포인트들은 같은 사후 확률을 가지게 될 것이기 때문이다. 


선형적으로 분리가 되는 데이터에 대해서는 데이터 포인트가 많다고 하더라도, 하나의 선호해를 찾아갈 수 없을 수 있다. 그래서, 사전확률을 포함시키고 w에 대한 [[최대사후분포 MAP(Maximum posterior]]해를 찾는 해결방식이 필요하다.

## 4.3.3 반복 재가중 최소 제곱법
선형 회귀 모델은 가우시안 노이즈 모델을 바탕으로 한 최대 가능도 해는 닫힌 상태였다. 이는 로그 가능도 함수가 매개변수 벡터 W 에 대해 이차 종속성을 가지고 있었기 때문임.

로지스틱 회귀 모델의 경우에는 로지스틱 시그모이드 함수의 비선형으로 인해서 해가 닫힌 형태가 아니다. 

하지만 이차식 형태로부터 그렇게 크게 다르지는 않다. 더 정확하게 말하자면 오류 함수는 볼록한==형태를 가지고 있으며, 다라서 유일한 최솟값을 가지고 있다.==

오류 함수는 [[뉴턴 라프슨 Newton-Raphson]] 반복 최적화 방법이라는 효율적인 반복 테크닉을 통해서 쉽게 최소화 할 수 있다.

오류함수의 최소화 방법: [[뉴턴 라프슨 Newton-Raphson]]

[[헤시안 행렬]]

[[순차적 경사 하강법]]

[[지수족]]


사후 클래스 확률을

특징 변수들의 선형함수에 대한 로지스틱 함수로 

표현 가능함.

단순한 형태의 사후확률이 모든 종류의 클래스 조건부 밀도 분포에 대해서 결과로 나오는 것은 아님.

[[프로빗함수]]

[[정준 연결 함수]]

[[라플라스 근사]]

[[정규화 상수 Z]]


[[소프트 맥스]]
