---
layout: page-sidenav
group: "Chapter. 4"
title: "0. Linear Models for Classification"
---

####### *날짜  2022-08-16 11:32*
 
태그: #선형분류 #DL

### PRML_4_선형 분류 모델(Linear Models for Classification)
---

앞 장에서는 회귀 모델에 대해서 살펴보았음.

현재 장에서는 분류 문제를 다루고자 함.

분류 문제는 입력 벡터 x 로 부터 이에 대응되는 이산( #discrete )값을 가지는 타겟 클래스 C 에 대해서 어떤 하나의 클래스에 속하도록 선정하는 작업임. 


K개의 클래스가 존재하므로, C_k로 표기함.

분류의 기본전제는 하나의 데이터가 다중의 클래스를 가지지 않는다는 것이다.
다시말해 클래스는 상호 배타적인 관계를 이루며, input space 를 각 영역으로 분리할 수 있다.

이렇게 나누어진 local 영역 각각을 #decision_region 이라고 한다.. 또한 이를 나누는 경계면을 #decision_boundary 또는 #decision_surface 라고 한다.

해당 단원에서는 선형 모델을 활용한 분류를 학습한다.
>입력 x 에 대한 선형 함수를 통해 분류를 수행하는 것이다.
>입력 벡터 x 가 D 차원 벡터라면, 이는 D-1 차원의 #hyperplane 들로 나눌 수 있다.
>위와 같이 선형 식으로 데이터의 차원을 분리할 수 있다면, 이 데이터는 선형 분리가 가능하다고 한다. #linearly_separable

회귀에서는 t 가 실수 범위의 값 또는 실수 벡터였으나, #classification 분류 문제에서는 클래스 레이블 ( #label )로 분류가 된다.

두 개의 클래스로 나누어지는 문제는 간단히  t∈0,1 으로 정의한다.

C1 에 속하는 경우 *t=1*로, 아닌 경우 *t=0*

클래스 개수가 K개 라면, t 를 크기가 K인 binary vector로 정의한다.( #onehot_vector )

따라서 K의 크기가 5이고 클래스 2 에 속하는 t는 아래와 같이 표현한다.
   
$$ {\bf t}=(0,1,0,0,0)^T \qquad{(4.1)} $$

#onehot_vector #1-to-K_binary_coding_scheme

이러한 방식을 *1-to-K binary coding scheme* 이라고 한다..

비확률 모델에서는 다른 방식으로 표현된다.

1장에서는 분류 문제를 3가지 접근법으로 서술한다.
[[1. Discriminant Functions]]
[[3. Prob. Discriminative Models]]
[[2. Prob. Generative Models]]

- 이러한 방식을 *1-to-K binary coding scheme* 이라고 한다. (이후에도 자주 언급된다.)
    - 물론 비확률 모델에서는 조금 다른 방식으로 이를 표현 할 것이다.

- 1장에서 분류 문제를 해결하는  3가지 접근법을 간단하게 서술하였다.
    - *Discriminant function*
    - *Discriminative model*
    - *Generative model*
- 이와 관련된 사항은 1.5.4 절을 참고할 것.
- 분류를 위한 가장 간단한 접근법은 판별 함수(discriminant function)을 사용하는 것
- 하지만 조건부 확률 분포를 사용하는 방법이 더 강력하다.
- 조건부 확률 분포 \\(p(C_k\|x) \\) 를 결정하기 위한 2가지 방법
    - 직접 모델링 : 데이터로부터 직접적으로 이 분포를 모델링한다. (즉, 사후분포 근사 방식)
    - Generative 접근 방식 : 사후 분로로 부터 사전 확률과 가능도 함수로 나누어 모델링하는 방식
    - 우리가 이미 잘 알고 있는 방식이다.

$$ p(C_k|{\bf x})=\dfrac{p({\bf x}|C_k)p(C_k)}{p({\bf x})} \qquad{(4.2)} $$

- 3장에서 살펴보았던 선형 회귀 모델을 응용해서 \\(y({\bf x}, {\bf w}) \\) 를 이용해 분류기를 작성할 수도 있다.
- 가장 간단한 분류기는 다음과 같은 형태를 취한다. 
    - \\(y({\bf x})={\bf w^Tx}+w_0 \\) 
    - 이 경우 \\(y \\)는 실수 값을 가지게 된다.
    - 2-class 문제에서는 분류를 사용하기 위해 이 값을 다시 (0,1) 범위를 가지는 값으로 변환하게 된다.
    - 이를 처리하기 위해 비선형 함수를 도입하게 된다.

$$ y({\bf x})=f({\bf w^Tx}+w_0) \qquad{(4.3)} $$

- 기계 학습 분야에서는 이러한 함수 \\(f() \\) 를 활성 함수( *activation function* ) 라고 부른다. 
    - 그리고 이에 대한 역함수를 *link function* 이라고 부른다.
- \\(y({\bf x})=constant \\) 에서 결정 평면이 이루어진다. (즉, \\({\bf w^Tx}+w_0=constant \\) )
    - \\(f() \\) 함수가 비선형일지라도 결정 경계는 \\(x \\) 에 대해 선형이 된다.
    - 이러한 이유로 식 위의 식을 *generalized linear model* 이라고 부른다.
- 논의를 시작하기에 앞서 3장에서 사용한 기저 함수(basis function)는 일단 도입하지 않기로 한다.
    - 즉, 입력 변수 \\(x \\) 를 바로 사용하는 형태로 식을 전개할 것이다.
    - 기저 함수를 도입하는 것은 좀 더 뒤에서 살펴보기로 한다.


### 출처(참고문헌)
- Pattern Recognition and Machine Learning = Christopher M. Bishop
- http://norman3.github.io/prml/

### 연결문서
[[0. Linear Models for Classification]]