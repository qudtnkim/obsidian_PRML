---
layout: page-sidenav
group: "Chapter. 4"
title: "0. Linear Models for Classification"
---

####### *날짜  2022-08-16 11:32*
 
태그: #선형분류 #DL

### PRML_4_선형 분류 모델(Linear Models for Classification)
---

앞 장에서는 회귀 모델에 대해서 살펴보았음.

현재 장에서는 분류 문제를 다루고자 함.

분류 문제는 입력 벡터 x 로 부터 이에 대응되는 이산( #discrete )값을 가지는 타겟 클래스 C 에 대해서 어떤 하나의 클래스에 속하도록 선정하는 작업임. 


K개의 클래스가 존재하므로, C_k로 표기함.

분류의 기본전제는 하나의 데이터가 다중의 클래스를 가지지 않는다는 것이다.
다시말해 클래스는 상호 배타적인 관계를 이루며, input space 를 각 영역으로 분리할 수 있다.

이렇게 나누어진 local 영역 각각을 #decision_region 이라고 한다.. 또한 이를 나누는 경계면을 #decision_boundary 또는 #decision_surface 라고 한다.

해당 단원에서는 선형 모델을 활용한 분류를 학습한다.
>입력 x 에 대한 선형 함수를 통해 분류를 수행하는 것이다.
>입력 벡터 x 가 D 차원 벡터라면, 이는 D-1 차원의 #hyperplane 들로 나눌 수 있다.
>위와 같이 선형 식으로 데이터의 차원을 분리할 수 있다면, 이 데이터는 선형 분리가 가능하다고 한다. #linearly_separable

회귀에서는 t 가 실수 범위의 값 또는 실수 벡터였으나, #classification 분류 문제에서는 클래스 레이블 ( #label )로 분류가 된다.

두 개의 클래스로 나누어지는 문제는 간단히  t∈0,1 으로 정의한다.

C1 에 속하는 경우 *t=1*로, 아닌 경우 *t=0*

클래스 개수가 K개 라면, t 를 크기가 K인 binary vector로 정의한다.( #onehot_vector )

따라서 K의 크기가 5이고 클래스 2 에 속하는 t는 아래와 같이 표현한다.
   
$$ {\bf t}=(0,1,0,0,0)^T \qquad{(4.1)} $$

#onehot_vector #1-to-K_binary_coding_scheme

이러한 방식을 *1-to-K binary coding scheme* 이라고 한다..

비확률 모델에서는 다른 방식으로 표현된다.

1장에서는 분류 문제를 3가지 접근법으로 서술한다.
[[1. Discriminant Functions]]
[[3. Prob. Discriminative Models]]
[[2. Prob. Generative Models]]

**(a) Generative Models**

- 클래스-조건부 밀도(class-conditional density)인 \( p({\bf x}\|C\_k) \\) 와 사전 확률 \\( p(C_k) \\) 를 각각 추론하여 사후 확률을 추론하거나,
- 결합 확률 \\( p({\bf x}, C\_k) \\) 의 주변화(marginalizing) 과정을 통해 사후 확률을 얻게 된다.

$$p(C_k|{\bf x})=\dfrac{p({\bf x}|C_k)p(C_k)}{p({\bf x})} \qquad{(1.82)}$$

$$p({\bf x})=\sum_{k}p({\bf x}|C_k)p(C_k) \qquad{(1.83)}$$

- 이름이 Generative 모델인 이유는 추론된 분포로 부터 임의적으로 새로운 데이터를 만들어낼 수 있기 때문이다.
    - 즉, 주어진 데이터를 통해 모델링된 분포로부터 완전히 새로운 샘플들을 재생성해 낼 수 있는 능력이 있다.
    - 모델이 개떡같이 추론되었다면 재샘플링 데이터가 원래 데이터와 유사하지 않을 가능성은 당연히 높음

**(b) Discriminative Models**

- 사후 확률(posterior)를 직접 근사하는 모델이다.
    - 앞서 설명한 Generative 모델은 사후 확률을 직접 구하는 것이 아니라 클래스-조건부 밀도와 사전 확률로 구분하여 간접적으로 추론을 하는 과정을 거쳤다.
    - 여기서는 직접적인 방법으로 해당 확률을 모델링한다.
    - 우선 이런게 있다고만 알고 이후에 4장을 참고하도록 한다.

**(c) Discriminant Function**

- 용어에 *discriminant* 가 들어가 있다고 해서 앞의 방식과 유사할 것이라 생각할 수도 있지만 전혀 그렇지 않다.
- 베이즈 확률 모델에 의존하지 않고 입력 공간을 바로 결정 모델에 대입하여 판별식을 찾아내는 방식이다.
    - 즉, 확률을 다루지 않으므로 사후 확률 등을 따지지 않는다.

- 모두 다 4장에서 다시 자세하게 설명하고 있으니 걱정하지 않아도 된다.

----

- 그냥 넘기기에는 좀 아쉽기 때문에 아주 간단히만 살펴보자.

- **(a). Generative model**
    - 입력 공간의 차원이 증가할 수록 좀 더 정확한 클래스-조건부 밀도를 구하기 위한 많은 샘플이 필요
    - 클래스에 대한 사전 확률 값은 샘플 수를 세기만 하면 되므로 상대적으로 구하기 쉬움
    - 새로운 데이터가 입력되었을 때 추정된 모델로부터 확률 값을 예측할 수 있으므로 낮은 확률값 등을 보고 이상치(outlier)를 확인할 수 있다.
    - 명시적, 암묵적으로 \\( p(x) \\) 의 분포를 모델링하게 된다.
    
- **(b). Discriminative model**
    - 사후 분포를 바로(direct) 추론한다.
    - 아래 그림을 보면 왼 쪽에 표기된 클래스-조건부 밀도는 상대적으로 복잡한데, 오른쪽 사후 분포는 매우 간단하다.
        - 즉, 클래스-조건부 밀도가 사후 분포에 그리 영향을 주지 않는다.
        - 이 경우에는 바로 사후 분포를 찾는게 더 편할 수 있다.
        - 클래스-조건부 밀도가 뭔지는 4장에 자세히 나온다.

<div class="text-center">
  <img src="{{ site.baseurl }}/images/Figure1.27a.png" alt="Figure 1.27a" width="180px" />
  <img src="{{ site.baseurl }}/images/Figure1.27b.png" alt="Figure 1.27b" width="180px" />
</div>

- **(c). Discriminant function**
    - 입력 공간을 결정 공간에 바로 매핑시키는 방식이다.
    - 위의 그림에서 녹색 선에 해당하는 방식이다. (이걸 바로 찾는다)
    - 확률을 다루지 않으므로 사후 확률 등을 추정하지 않는다.

- (a)와 (b) 에서는 사후 확률을 주요하게 다루고 있으나 (c)는 그렇지 않다.
    - 하지만 (c) 와 같은 방식을 사용하더라도 여전히 사후 확률을 예측하는 것은 큰 의미가 있다.

- 사후 확률 분포를 활용하는 벙법에 대해서는 간단히 언급만 하고 넘어가도록 한다.
- **Minimizing Risk**
    - 앞서 살펴본 Loss 행렬이 고정된 값이 아니라 시간에 따라 바뀔 수 있다고 생각해보자.
    - 만약 우리가 사후 확률을 알고 있다면 이를 쉽게 응용할 수 있다.
- **Reject Option**
    - 분류 선택을 유보할 수 있는 영역을 만들 수 있다. 앞 절에서 살펴봤다.
- **Compensating for class priors**
    - 보통 업데이트 방식의 확률 모델에서 베이지안 이론을 사용하는 경우, 얻어진 사후 확률을 다시 사전 확률로 가정하여 새로운 데이터에 대해 적용 가능함.
    - 이런 방식은 이후 베이지안 방식에서 다루게 될 것이다.
- **Combining models**
    - 보통 복잡한 문제는 좀 더 작은 문제로 나누어 해결하고 조합하는 방식이 선호됨
    - 확률 모델에서도 좀 더 간단한 모델을 취하고 싶은 경우, 각각의 요소들을 독립적이라 가정하고 식을 전개하게 된다.
    - 이런 모델을 조건부 독립이라고 한다.
    - 사후 분포를 이용한 식도 이와 마찬가지로 전개가 가능하다.
    - 하나의 어려운 결합 확률을 구하지 말고 모델링이 쉬운 두 개의 사후 확률을 구해 이를 결합하자는 것
    - 아래 식을 참고하면 된다.
    
$$p({\bf x}_I, {\bf x}_B | C_k) = p({\bf x}_I |\ C_k)p({\bf x}_B | C_k)$$
    
$$p(C_k | {\bf x}_I, {\bf x}_B) \propto p({\bf x}_I, {\bf x}_B | C_k)p(C_k) \propto p({\bf x}_I | C_k)p({\bf x}_B | C_k)p(C_k) \propto \dfrac{p(C_k | {\bf x}_I)p(C_k|{\bf x}_B)}{p(C_k)} \qquad{(1.85)}$$









- 이와 관련된 사항은 1.5.4 절을 참고할 것.
- 분류를 위한 가장 간단한 접근법은 판별 함수(discriminant function)을 사용하는 것
- 하지만 조건부 확률 분포를 사용하는 방법이 더 강력하다.
- 조건부 확률 분포 \\(p(C_k\|x) \\) 를 결정하기 위한 2가지 방법
    - 직접 모델링 : 데이터로부터 직접적으로 이 분포를 모델링한다. (즉, 사후분포 근사 방식)
    - Generative 접근 방식 : 사후 분로로 부터 사전 확률과 가능도 함수로 나누어 모델링하는 방식
    - 우리가 이미 잘 알고 있는 방식이다.

$$ p(C_k|{\bf x})=\dfrac{p({\bf x}|C_k)p(C_k)}{p({\bf x})} \qquad{(4.2)} $$

- 3장에서 살펴보았던 선형 회귀 모델을 응용해서 \\(y({\bf x}, {\bf w}) \\) 를 이용해 분류기를 작성할 수도 있다.
- 가장 간단한 분류기는 다음과 같은 형태를 취한다. 
    - \\(y({\bf x})={\bf w^Tx}+w_0 \\) 
    - 이 경우 \\(y \\)는 실수 값을 가지게 된다.
    - 2-class 문제에서는 분류를 사용하기 위해 이 값을 다시 (0,1) 범위를 가지는 값으로 변환하게 된다.
    - 이를 처리하기 위해 비선형 함수를 도입하게 된다.

$$ y({\bf x})=f({\bf w^Tx}+w_0) \qquad{(4.3)} $$

- 기계 학습 분야에서는 이러한 함수 \\(f() \\) 를 활성 함수( *activation function* ) 라고 부른다. 
    - 그리고 이에 대한 역함수를 *link function* 이라고 부른다.
- \\(y({\bf x})=constant \\) 에서 결정 평면이 이루어진다. (즉, \\({\bf w^Tx}+w_0=constant \\) )
    - \\(f() \\) 함수가 비선형일지라도 결정 경계는 \\(x \\) 에 대해 선형이 된다.
    - 이러한 이유로 식 위의 식을 *generalized linear model* 이라고 부른다.
- 논의를 시작하기에 앞서 3장에서 사용한 기저 함수(basis function)는 일단 도입하지 않기로 한다.
    - 즉, 입력 변수 \\(x \\) 를 바로 사용하는 형태로 식을 전개할 것이다.
    - 기저 함수를 도입하는 것은 좀 더 뒤에서 살펴보기로 한다.


### 출처(참고문헌)
- Pattern Recognition and Machine Learning = Christopher M. Bishop
- http://norman3.github.io/prml/

### 연결문서
[[0. Linear Models for Classification]]