- Binary Classification 의 경우에, 조건부 분포에 대하여 $C_{1}$ 의 사후확률을 x 의 선형 함수에 대한 로지스틱 시그모이드 함수로 표현함.
- *p*(x|*$C_{k}$)



- 클래스가 여럿인 경우에는 K 번째 클래스에 대한 사후확률을 표현할 때, x에 대한 선형함수를 소프트맥스 함수로 변환하여 표현함.

- 클래스 조건부 밀도가 특정한 분포를 가지고 있을 경우에는 [[최대가능도]] 방법을 이용하여 밀도의 매개변수와 클래스 사전 분포  P(Ck)를 구하고, 거기에 [[베이지안 정리]]를 적용해서 사후 클래스 확률을 구함.

- 대안으로는 일반화된 선형 모델의 함수 형태를 명시적으로 사용하고, 여기에 최대 가능도 방법을 적용해서 함수의 매개변수를 직접 구함. [[반복 재가중 최소 제곱법]]

일반화된 선형 모델의 매개변수를 찾을 때, 클래스 조건부 밀도와 클래스 사전 분포를 따로 피팅하고 [[베이지안 정리]] 를 적용하여 간접적으로 매개변수를 찾는 것은 [[생성적 모델링]] 의 예시이다.

이에 반해서 직접적인 접근법으로는 조건부 분포 $p(C_{k}|x)$  를 통해 정의된 #가능도함수 를 직접 극대화 한다. 이는 [[판별적 훈련]]의 예시 이다.

---------------

## 고정된 [[기저 함수]]

원입력 벡터 x 에 대해 직접 적용되는 분류 모델들은 기저 함수들의 벡터를 이용하면, 입력 값에 대해 고정된 비선형 변환을 먼저 적용한 후에도 사용할 수 있다. 이결과로 얻어지는 결정 경계는 특징 공간상에서 선형이며, 이 결정 경계는 원 x 공간에서는 비선형일 것 이다. (*이게 뭔소리냐*)

특징 공간 상에서 선형적으로 분리 가능한 클래스들이 원래의 관측 공간에서도 선형적으로 분리되는 것은 아니다.(*당연*)

회귀를 위한 선형 모델에 대한 논의에서 처럼 기저 함수들 중 하나는 보통 특징공간 $파이_{o}(x)=1$
과 같이 상숫값을 가지도록 설정된다.

![[Pasted image 20220829124444.png]]
그림 4.12 는 비선형 기저 함수에서 변환된 데이터가 선형 판별되는 것을 묘사한다.

p($C_{2}|\phi$) = 1 - p($C_{1}|\phi$)
$$p(C_{1}|\phi) = y(\phi) = \sigma(w^T\phi)$$

$\sigma( )$ 는 로지스틱 시그모이드 함수임. 
위의 모델은 [[로지스틱 회귀]]라고 불리며, 분류를 위한 모델임.

[[최대가능도]] 방법을 이용해서 로지스틱 회귀 모델의 매개변수를 구하면, 이를 위해서는 로지스틱 시그모이드 함수의 미분값을 이용해야함. 로지 스틱 시그모이드의 미분값에는 시그모이드 함수의 미분값이 포함된다.

$$d\sigma/da = \sigma(1-\sigma)$$
n = 1 ,...,N 에 대해서 $t_{n}$ 은 {0,1} 
타겟 클래스가 0 혹은 1인 문제에서 $\phi_{n} = \phi(x_{n})$ 인 데이터 집합 들에서 {$\phi(x_{n}), t_{n}$} 에 대해서 가능도 함수를 적는 다면 다음과 같다.
![[Pasted image 20220829151710.png]]


크로스 엔트로피의 곱
(($\pi$) 는  요소 곱을 의미한다.)

$y_{n} = p(C_{1}|\Phi_{n})$

기저함수를 만족할 때, 클래스 1 일 확률이 $y_{n}$ 임

가능도 함수의 음의 로그값을 취하면 오류 함수가 정의 됨. -> cross entropy 함수임

선형적으로 분리 가능한 데이터 집합에 대해 최대 가능도 방법을 사용하면 심각한 과적합 문제를 겪는다. 

$\sigma = 0.5$ 에 해당하는 초공간이 두 클래스를 나누는 경우에 발생한다. 이 때, $w^{T}\phi = 0$ 에 해당하며, w 의 크기는 무한대가 된다. 이 경우 특징 공간 상에서의 로지스틱 회귀 함수는 무한대로 가팔라지며, 따라서 헤비사이드 계단 함수가 된다. 각각의 클래스 k 에서 온 모든 훈련 포인트 들이 [[사후 확률]] p($C_{k}|x)=1$ 을 가지게 됨.  

또한, 해들의 연속체가 존재하게 되는데, 초공간이 클래스들을 나눈다 하더라도 훈련 집합의 데이터 포인트들은 같은 사후 확률을 가지게 될 것이기 때문이다. 


선형적으로 분리가 되는 데이터에 대해서는 데이터 포인트가 많다고 하더라도, 하나의 선호해를 찾아갈 수 없을 수 있다. 그래서, 사전확률을 포함시키고 w에 대한 [[최대사후분포 MAP(Maximum posterior]]해를 찾는 해결방식이 필요하다.

## 4.3.3 반복 재가중 최소 제곱법
선형 회귀 모델은 가우시안 노이즈 모델을 바탕으로 한 최대 가능도 해는 닫힌 상태였다. 이는 로그 가능도 함수가 매개변수 벡터 W 에 대해 이차 종속성을 가지고 있었기 때문임.

로지스틱 회귀 모델의 경우에는 로지스틱 시그모이드 함수의 비선형으로 인해서 해가 닫힌 형태가 아니다. 

하지만 이차식 형태로부터 그렇게 크게 다르지는 않다. 더 정확하게 말하자면 오류 함수는 볼록한==형태를 가지고 있으며, 다라서 유일한 최솟값을 가지고 있다.==

오류 함수는 [[뉴턴 라프슨 Newton-Raphson]] 반복 최적화 방법이라는 효율적인 반복 테크닉을 통해서 쉽게 최소화 할 수 있다.

오류함수의 최소화 방법: [[뉴턴 라프슨 Newton-Raphson]]

[[헤시안 행렬]]

[[순차적 경사 하강법]]

[[지수족]]


사후 클래스 확률을

특징 변수들의 선형함수에 대한 로지스틱 함수로 

표현 가능함.

단순한 형태의 사후확률이 모든 종류의 클래스 조건부 밀도 분포에 대해서 결과로 나오는 것은 아님.

[[프로빗함수]]

[[정준 연결 함수]]
